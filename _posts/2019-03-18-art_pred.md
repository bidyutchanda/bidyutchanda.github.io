---
layout: post
title: Predicting type of art using CNNs and FastAI
subtitle: Is it a painting, a drawing or a sculpture?
show-avatar: false
bigimg: /img/jj-ying-215319-unsplash.jpg
share-img: /img/jj-ying-215319-unsplash.jpg
gh-repo: bidyutchanda/art-prediction-FastAI
gh-badge: [star, fork, follow]
tags: [cnn,fastai,image recognition]
---

## About the Project
When I started knowing more about Deep Learning and had gone through sufficient number of blog posts stating their own learning paths for this specific area, a man called Jeremy Howard popped up quite a number of times and how he and Rachel Thomas made a practical DL course using their own library, [FastAI](https://fast.ai). Now, many of the readers here would have suggested me otherwise as many of the blog posts did, to start (obviously) with Andrew Ng's **deeplearning.ai** specialization. But being a more getting-my-hands-dirty-and-learning-on-the-field type of person, I chose the former to start with. Do not get me wrong, I will soon be shifting to Ng's Coursera specialization, almost the Bible of DL at present. But, FastAI is something, and which I liked the most, that does not go into nitty-gritties of things, but will provide you with the best possible result. 

**This is my first project based on Convolutional Neural Networks using FastAI and predicts the type of art when inputted with an image, whether it is a painting, a drawing, a sculpture, an iconography or an engraving.**



## Project Details
- The Kaggle notebook for this project to fork is linked [here](https://www.kaggle.com/bidyutchanda/art-prediction-using-cnn-fastai-acc-94-74).
- The dataset for 5000 images divided into 5 categories is taken from a [Kaggle source](https://www.kaggle.com/moosecat/art-images-drawings-painting-sculpture-engraving).
- The Github repo for this can be accessed from [here](https://github.com/bidyutchanda/art-prediction-FastAI).
- Python libraries used extensively are :
    - [fastai](https://github.com/fastai/fastai) 
    - [ImageDataBunch](https://docs.fast.ai/vision.data.html#ImageDataBunch) 
    - [cnn_learner](https://docs.fast.ai/vision.learner.html#cnn_learner) 



## Learning and Prediction
_Disclaimer: There are some steps below which are Kaggle specific only and will be mentioned as such beforehand_.
### A. Importing the libraries
Both Kaggle and Google Colab comes pre-installed with `fastai` and `pytorch`. But if you are doing this on your local computer for the first time, you have to download `pytorch` and `fastai`, in that order, preferably on a virtual environment like `conda`.

More on downloading Pytorch is available on their [website](https://pytorch.org/get-started/locally/).

After `pytorch` is installed, run `conda install -c fastai fastai` on a Linux terminal to get started with this project. 


```python
#Imports all the libraries used in the project
from fastai import *
from fastai.vision import *
```



### B. Declaring PATH 
Take `PATH` to be the parent folder of the training and validation set. 

In Kaggle, this is the folder. If on local system, the PATH will be different. 



```python
PATH = "../input/art-images-drawings-painting-sculpture-engraving/art_dataset_cleaned/art_dataset"
```



### C. Accessing the images and labels
If we take a look at the dataset, we will notice that the label names are actually the folders, i.e., all drawings are contained in a folder called _drawings_, all paintings in a folder called _paintings_ and so on, in an ImageNet style. And also, the training and validation set have already been made by the dataset provider. 

But before that, we have to declare how we are going to handle the image transforms. FastAI provides with a function called `get_tranforms()` within which we can mention how we transform the images, as here, we do not want our CNN to be trained on images which are flipped in any way. That means if a drawing is flipped or rotated in any way or to any degree, that image becomes a completely different picture and we do not want our CNN to learn that. Maybe in case of satellite images of land and sea, we can flip the images because then, the image does not become a totally different one. 



```python
tfms = get_transforms(do_flip=False)
#do_flip=False because of reason mentioned above. 
data = ImageDataBunch.from_folder(PATH, train="training_set", valid="validation_set", ds_tfms=tfms, size=200, num_workers=0)
#We direct train and valid to the respective sets inside the folder. 
```



Now we check the data whether it is loaded properly or not. 



```python
data.show_batch(rows=3, figsize=(5,5))
```


<br>
![showbatch](/img/showbatch.png)


`data.classes` lists the classes or the labels of the images. 



```python
['drawings', 'engraving', 'iconography', 'painting', 'sculpture']
```



### D. Downloading trained weights from Resnet34 or Resnet50
This is a Kaggle specific step. Since Kaggle kernels are read-only, so we add trained Resnet34 model to our kernel by selecting _Add Data_ from the right hand section of the screen and we also make a directory for copying the .pth model to our filesystem. 



```python
cache_dir = os.path.expanduser(os.path.join('~','.torch'))
#First make a ~/.torch folder if not available. 
if not os.path.exists(cache_dir):
    os.makedirs(cache_dir)
models_dir = os.path.join(cache_dir,'models')
#then make ~/.torch/models, if not already available.
if not os.path.exists(models_dir):
    os.makedirs(models_dir)
```


Then we copy the .pth folder to our filesystem. 



```python
!cp ../input/resnet34/resnet34.pth ~/.torch/models/resnet34-333f7ec4.pth 
```


And we declare another PATH variable to be used in the CNN learner. 



```python
MODEL_PATH = '/tmp/models'
```



### E. Training the CNN to learn from `data`
If you are not using Kaggle, ignore the whole section D and start again from this step. 

So now, we declare our `cnn_learner` to be trained from `data` by taking training and validation images from `PATH` previously declared. 



```python
learn = cnn_learner(data, models.resnet34, metrics=accuracy, model_dir=MODEL_PATH)
#Ignore model_dir attribute if on local system. It is a Kaggle specific step again.
```



Now we use `fit_one_cycle()` and mention the no. of epochs we want it to train the first time. 



```python
learn.fit_one_cycle(4)
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
        <th>epoch</th>
        <th>train_loss</th>
        <th>valid_loss</th>
        <th>accuracy</th>
        <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
        <th>0</th>
        <th>0.489632</th>
        <th>0.265858</th>
        <th>0.901869</th>
        <th>02:02</th>
    </tr>
    <tr>  
        <th>1</th>
        <th>0.301034</th>
        <th>0.202761</th>
        <th>0.925234</th>
        <th>01:56</th>
    </tr>
    <tr>
        <th>2</th>
        <th>0.233835</th>
        <th>0.189954</th>
        <th>0.929907</th>
        <th>01:57</th>
    </tr>
    <tr>
        <th>3</th>
        <th>0.199139</th>
        <th>0.181202</th>
        <th>0.934579</th>
        <th>01:56</th>
    </tr>
  </tbody>
</table>
</div>
